#!/bin/bash
#SBATCH --job-name=motus_eval
#SBATCH --output=/home/lfeng/task_logs/%j.log
#SBATCH --partition=h100
#SBATCH --qos=vita
#SBATCH --ntasks=4
#SBATCH --cpus-per-task=16
#SBATCH --gpus-per-node=1
#SBATCH --nodes=4
#SBATCH --time=12:00:00
#SBATCH --mem=200G

# ============================================================
# Evaluate Motus policy on all 24 subsets (train+test)
# using 4 nodes with 1 GPU each (6 configs per node).
# ============================================================

SCRIPT_PATH="/work/vita/lanfeng/vlas/Motus/inference/standalone/motus_eval.py"
RESULT_DIR="/work/vita/lanfeng/vlas/motus_eval_result"
MOTUS_CKPT="/work/vita/lanfeng/vlas/Motus/checkpoints/wise_dataset/motus_wise_dataset/checkpoint_step_15000/pytorch_model"
DATASET_ROOT="/work/vita/lanfeng/vlas/vla/wise_dataset/no_noise_demo_1_round"
CONFIG_FILE="/work/vita/lanfeng/vlas/Motus/configs/wise_dataset_test_loss.yaml"

mkdir -p $RESULT_DIR

# Each srun task gets SLURM_PROCID in {0..3}.
# Compute the subset shard: 6 configs per task (24 total configs).
srun --ntasks=4 --ntasks-per-node=1 bash -c '
    START=$((SLURM_PROCID * 6))
    END=$(((SLURM_PROCID + 1) * 6))
    echo "[Node $(hostname), PROCID=$SLURM_PROCID] Evaluating subsets $START to $((END - 1))"

    # Distribute standard CUDA devices so they do not overlap
    export CUDA_VISIBLE_DEVICES=0

    python '"$SCRIPT_PATH"' \
        --config='"$CONFIG_FILE"' \
        --ckpt_path='"$MOTUS_CKPT"' \
        --dataset_root='"$DATASET_ROOT"' \
        --result_dir='"$RESULT_DIR"' \
        --start_subset=$START \
        --end_subset=$END \
        --split=both \
        --eval_rounds=1 \
        > "'"$RESULT_DIR"'/node_${SLURM_PROCID}.log" 2>&1
'

echo "All rollouts complete."

# Final aggregation (runs on the first node)
echo "Running final aggregation..."
python $SCRIPT_PATH \
    --aggregate_only \
    --result_dir=$RESULT_DIR \
    --ckpt_path="" --wan_path="" --vlm_path=""

echo "Done. Results in $RESULT_DIR"
