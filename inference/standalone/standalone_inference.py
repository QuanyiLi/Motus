import torch
import torch.nn as nn
import numpy as np
import cv2
from pathlib import Path
import sys
import os
import logging
from typing import List, Dict, Any, Optional
from collections import deque
from PIL import Image
from transformers import AutoProcessor
import matplotlib.pyplot as plt
import matplotlib

matplotlib.use('Agg')  # Use non-interactive backend

# Setup path for local dependencies (models, utils, bak)
CURRENT_DIR = Path(__file__).resolve().parent
if str(CURRENT_DIR) not in sys.path:
    sys.path.append(str(CURRENT_DIR))
if str(CURRENT_DIR / "models") not in sys.path:
    sys.path.append(str(CURRENT_DIR / "models"))
if str(CURRENT_DIR / "bak") not in sys.path:
    sys.path.insert(0, str(CURRENT_DIR / "bak"))

try:
    from models.motus import Motus, MotusConfig
    from wan.modules.t5 import T5EncoderModel
    from utils.image_utils import resize_with_padding
except ImportError as e:
    raise ImportError(f"Failed to import local dependencies. Make sure 'models', 'utils', and 'bak' folders are in {CURRENT_DIR}. Error: {e}")

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class StandaloneMotusPolicy:
    """
    Standalone Motus Policy for inference in external simulation environments.
    Generates 48 actions and executes the first 20 (action chunking).
    """
    def __init__(
        self,
        checkpoint_path: str,
        wan_path: str,
        vlm_path: str,
        device: str = "cuda",
        state_dim: int = 9,
        action_dim: int = 8,
        video_height: int = 384,
        video_width: int = 320,
        execute_steps: int = 20,
        stat_path: Optional[str] = None,
        **kwargs
    ):
        self.device = device
        self.checkpoint_path = checkpoint_path
        self.wan_path = wan_path
        self.vlm_path = vlm_path
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.video_height = video_height
        self.video_width = video_width
        self.execute_steps = execute_steps
        self.kwargs = kwargs

        # Initialize model WITHOUT loading pretrained backbones
        self.model = self._load_model()

        # Initialize VLM processor from vlm_path (for tokenization only)
        self.vlm_processor = AutoProcessor.from_pretrained(self.vlm_path, trust_remote_code=True)

        # Initialize caches
        self.obs_cache = deque(maxlen=1)
        self.action_queue = deque()

        # Model state
        self.current_state = None
        self.current_state_norm = None

        # T5 Embedding (must be loaded explicitly now)
        self.current_t5_embedding = None

        # Load normalization stats if provided
        self.stat_path = stat_path
        self.action_min = None
        self.action_max = None
        self.action_range = None
        self._load_normalization_stats()

        logger.info("Standalone Motus Policy initialized successfully.")

    def set_t5_embedding(self, pt_path: str):
        """Load precomputed T5 embedding generated by pregenerate_t5_all.py."""
        if not os.path.exists(pt_path):
            raise FileNotFoundError(f"T5 embedding file not found at {pt_path}")

        emb = torch.load(pt_path, map_location="cpu")
        if not isinstance(emb, torch.Tensor):
            emb = torch.tensor(emb)
        if emb.ndim == 2:
            emb = emb.unsqueeze(0)

        self.current_t5_embedding = emb.float()
        logger.info(f"Loaded T5 embedding from {pt_path}")

    def _load_model(self) -> Motus:
        logger.info("Initializing Motus model from scratch (no pretrained backbones)")
        config = self._create_model_config()
        model = Motus(config).to(self.device)

        try:
            logger.info(f"Loading checkpoint from {self.checkpoint_path}")
            model.load_checkpoint(self.checkpoint_path, strict=False)
            logger.info("Model checkpoint loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            raise

        model.eval()
        return model

    def _create_model_config(self) -> MotusConfig:
        vae_path = os.path.join(self.wan_path, "Wan2.2_VAE.pth")

        config_args = dict(
            wan_checkpoint_path=self.wan_path,
            vae_path=vae_path,
            wan_config_path=self.wan_path,
            video_precision='bfloat16',
            vlm_checkpoint_path=self.vlm_path,

            und_expert_hidden_size=512,
            und_expert_ffn_dim_multiplier=4,
            und_expert_norm_eps=1e-5,
            und_layers_to_extract=None,
            vlm_adapter_input_dim=2048,
            vlm_adapter_projector_type="mlp3x_silu",

            num_layers=30,
            action_state_dim=self.state_dim,
            action_dim=self.action_dim,
            action_expert_dim=1024,
            action_expert_ffn_dim_multiplier=4,
            action_expert_norm_eps=1e-6,

            global_downsample_rate=1,
            video_action_freq_ratio=6,
            num_video_frames=8,
            video_loss_weight=1.0,
            action_loss_weight=1.0,

            batch_size=1,
            video_height=self.video_height,
            video_width=self.video_width,

            load_pretrained_backbones=False,
            training_mode='finetune',
        )
        
        # Override with any custom kwargs for dynamic evaluation
        config_args.update(self.kwargs)
        
        config = MotusConfig(**config_args)
        return config

    def _load_normalization_stats(self):
        if self.stat_path and os.path.exists(self.stat_path):
            import json
            with open(self.stat_path, 'r') as f:
                stat_data = json.load(f)

            # Prefer wise_bot
            stats = None
            if "wise_bot" in stat_data:
                stats = stat_data["wise_bot"]
            elif "robotwin2" in stat_data:
                stats = stat_data["robotwin2"]
            else:
                stats = list(stat_data.values())[0] if stat_data else None

            if stats:
                min_vals = torch.tensor(stats['min'], dtype=torch.float32, device=self.device)
                max_vals = torch.tensor(stats['max'], dtype=torch.float32, device=self.device)
                # Expand rank-0/scalar ranges logic (like wise_bot [-1, 1]) to action_dim
                if min_vals.numel() == 1 and self.action_dim > 1:
                    min_vals = min_vals.repeat(self.action_dim)
                    max_vals = max_vals.repeat(self.action_dim)
                elif min_vals.numel() != self.action_dim:
                    logger.warning(f"Stat min dim {min_vals.numel()} mismatch action_dim {self.action_dim}. Resizing...")
                    if min_vals.numel() > self.action_dim:
                        min_vals = min_vals[:self.action_dim]
                        max_vals = max_vals[:self.action_dim]
                    else:
                        fill_min = min_vals[-1].repeat(self.action_dim - min_vals.numel())
                        fill_max = max_vals[-1].repeat(self.action_dim - max_vals.numel())
                        min_vals = torch.cat([min_vals, fill_min])
                        max_vals = torch.cat([max_vals, fill_max])

                self.action_min = min_vals
                self.action_max = max_vals
                self.action_range = self.action_max - self.action_min
                logger.info(f"Loaded normalization stats from {self.stat_path}")
            else:
                logger.warning("No valid stats found in stat file.")
        else:
            logger.warning("No stat file provided or found. Assuming actions/states are already normalized or don't need it.")

    def _normalize_actions(self, x: torch.Tensor) -> torch.Tensor:
        if self.action_min is None:
            return x
        shape = x.shape
        x_flat = x.reshape(-1, shape[-1])

        # Match dimensions smoothly for states vs actions
        feat_dim = shape[-1]
        a_min = self.action_min
        a_range = self.action_range

        if a_min.shape[0] < feat_dim:
            # Pad with the last element if state_dim > action_dim (e.g. 9 vs 8)
            pad_len = feat_dim - a_min.shape[0]
            a_min = torch.cat([a_min, a_min[-1].repeat(pad_len)])
            a_range = torch.cat([a_range, a_range[-1].repeat(pad_len)])
        elif a_min.shape[0] > feat_dim:
            # Truncate if slicing
            a_min = a_min[:feat_dim]
            a_range = a_range[:feat_dim]

        norm = (x_flat - a_min.unsqueeze(0)) / a_range.unsqueeze(0)
        return norm.reshape(shape)

    def _denormalize_actions(self, y: torch.Tensor) -> torch.Tensor:
        if self.action_min is None:
            return y
        shape = y.shape
        y_flat = y.reshape(-1, shape[-1])

        feat_dim = shape[-1]
        a_min = self.action_min
        a_range = self.action_range

        if a_min.shape[0] < feat_dim:
            pad_len = feat_dim - a_min.shape[0]
            a_min = torch.cat([a_min, a_min[-1].repeat(pad_len)])
            a_range = torch.cat([a_range, a_range[-1].repeat(pad_len)])
        elif a_min.shape[0] > feat_dim:
            a_min = a_min[:feat_dim]
            a_range = a_range[:feat_dim]

        denorm = y_flat * a_range.unsqueeze(0) + a_min.unsqueeze(0)
        return denorm.reshape(shape)

    def update_obs(self, image: np.ndarray, state: np.ndarray):
        """
        Update the observation.
        image: RGB image of shape (B, H, W, 3)
        state: array of shape (B, state_dim)
        """
        target_size = (self.video_height, self.video_width)

        if isinstance(image, np.ndarray):
            # Ensure 4D (B, H, W, C)
            if image.ndim == 3:
                image = np.expand_dims(image, axis=0)

            # [B, H, W, C] -> [B, C, H, W]
            image_tensor = torch.from_numpy(image).permute(0, 3, 1, 2)
        else:
            image_tensor = image.clone().detach() if torch.is_tensor(image) else torch.tensor(image)
            if image_tensor.dim() == 3:
                image_tensor = image_tensor.unsqueeze(0)
            if image_tensor.shape[-1] == 3: # Assuming B, H, W, C
                image_tensor = image_tensor.permute(0, 3, 1, 2)

        B = image_tensor.shape[0]

        # Resize if necessary
        if image_tensor.shape[-2:] != target_size:
            resized_tensors = []
            for i in range(B):
                img_np = image_tensor[i].permute(1, 2, 0).cpu().numpy()
                resized_np = resize_with_padding(img_np, target_size)
                if resized_np.dtype == np.uint8:
                    resized_np = resized_np.astype(np.float32) / 255.0
                resized_tensors.append(torch.from_numpy(resized_np).permute(2, 0, 1))
            image_tensor = torch.stack(resized_tensors, dim=0)
        elif image_tensor.dtype == torch.uint8:
            image_tensor = image_tensor.float() / 255.0

        self.obs_cache.append(image_tensor.to(self.device).float())

        if isinstance(state, np.ndarray):
            if state.ndim == 1:
                state = np.expand_dims(state, axis=0)
            state_tensor = torch.from_numpy(state).float()
        else:
            state_tensor = state.float()
            if state_tensor.dim() == 1:
                state_tensor = state_tensor.unsqueeze(0)

        self.current_state = state_tensor.to(self.device)
        self.current_state_norm = self._normalize_actions(self.current_state).to(self.device)

    def _tensor_to_pil_image(self, tensor_chw: torch.Tensor) -> Image.Image:
        """Helper to convert float tensor to PIL Image"""
        if tensor_chw.dtype != torch.float32:
            tensor_chw = tensor_chw.float()
        tensor_chw = tensor_chw.clamp(0, 1)
        np_img = (tensor_chw.permute(1, 2, 0).numpy() * 255.0).astype(np.uint8)
        return Image.fromarray(np_img, mode='RGB')

    def _preprocess_vlm_messages(self, instruction: str, image: Image.Image) -> Dict[str, torch.Tensor]:
        messages = [
            {
                'role': 'user',
                'content': [
                    {'type': 'text', 'text': instruction},
                    {'type': 'image', 'image': image},
                ]
            }
        ]
        text = self.vlm_processor.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)
        encoded = self.vlm_processor(text=[text], images=[image], return_tensors='pt')
        vlm_inputs = {
            'input_ids': encoded['input_ids'].to(self.device),
            'attention_mask': encoded['attention_mask'].to(self.device),
            'pixel_values': encoded['pixel_values'].to(self.device),
            'image_grid_thw': encoded.get('image_grid_thw', None)
        }
        if vlm_inputs['image_grid_thw'] is not None:
            vlm_inputs['image_grid_thw'] = vlm_inputs['image_grid_thw'].to(self.device)
        return vlm_inputs

    def act(self, image: np.ndarray, state: np.ndarray) -> np.ndarray:
        """
        Main entry point for external environments.
        Updates observation and returns a batch of actions.
        Returns: [B, action_dim] array 
        """
        self.update_obs(image, state)

        if len(self.action_queue) == 0:
            logger.info("Action queue empty. Replanning...")
            self._replan()

        # Pop the next action chunk [B, action_dim]
        return self.action_queue.popleft()

    def _replan(self):
        """Run the model to generate a new chunk of actions."""
        if self.current_t5_embedding is None:
            raise ValueError("No T5 embedding set. Call `set_t5_embedding` before running inference.")

        current_frame_batch = self.obs_cache[-1] # [B, C, H, W]
        B = current_frame_batch.shape[0]

        scene_prefix = ("The whole scene is in a realistic, industrial art style with three views: "
                        "a fixed rear camera, a movable left arm camera, and a movable right arm camera. "
                        "The aloha robot is currently performing the following task: ")

        # We can just construct one text string per batch item
        full_instruction_batch = [f"{scene_prefix}Perform the task."] * B

        # Expand precomputed T5 embedding to batch size if it is shape [1, L, D] and B > 1
        t5_emb = self.current_t5_embedding
        if t5_emb.shape[0] == 1 and B > 1:
            t5_emb = t5_emb.expand(B, -1, -1)

        t5_list = [t5_emb[i] for i in range(B)]

        first_frame_pil_batch = [self._tensor_to_pil_image(current_frame_batch[i].cpu()) for i in range(B)]

        # Build batched VLM inputs via loop
        vlm_inputs_list = []
        for i in range(B):
            vlm_input = self._preprocess_vlm_messages(full_instruction_batch[i], first_frame_pil_batch[i])
            vlm_inputs_list.append(vlm_input)

        # The underlying model handles batch via the vlm_inputs format
        num_inference_steps = 10
        with torch.no_grad():
            predicted_frames, predicted_actions = self.model.inference_step(
                first_frame=current_frame_batch,
                state=self.current_state,
                num_inference_steps=num_inference_steps,
                language_embeddings=t5_list,
                vlm_inputs=vlm_inputs_list,
            )

        # predicted_actions is shape [B, chunk_size, action_dim]
        actions_real = predicted_actions.cpu().numpy() # [B, chunk_size, action_dim]

        # Keep only the first `execute_steps` actions
        kept_actions = actions_real[:, :self.execute_steps, :] # [B, execute_steps, action_dim]

        logger.info(f"Generated {actions_real.shape[1]} actions per env. Keeping first {kept_actions.shape[1]}.")

        self.action_queue.clear()

        # We want to queue them such that popleft() returns [B, action_dim] slice
        for step_idx in range(kept_actions.shape[1]):
            self.action_queue.append(kept_actions[:, step_idx, :])

    def reset(self):
        self.obs_cache.clear()
        self.action_queue.clear()
        self.current_state = None


def verify():
    print("Starting standalone inference verification...")
    # These are dummy paths / stats if not provided, just testing model initialization and forward pass
    # ckpt_path = "/work/vita/lanfeng/vlas/Motus/ckpt_original_wan/wise_dataset/motus_wise_dataset/checkpoint_step_5000/pytorch_model/mp_rank_00_model_states.pt"
    # wan_path = "/work/vita/lanfeng/vlas/Motus/pretrained_models/Wan2.2-TI2V-5B"
    # vlm_path = "/work/vita/lanfeng/vlas/Motus/pretrained_models/Qwen3-VL-2B-Instruct"

    ckpt_path = "/home/quanyi/motus_original_wan/mp_rank_00_model_states.pt"
    wan_path = "/home/quanyi/code/Motus/pretrained_models/Wan2.2-TI2V-5B"
    vlm_path = "/home/quanyi/code/Motus/pretrained_models/Qwen3-VL-2B-Instruct"

    try:
        policy = StandaloneMotusPolicy(
            checkpoint_path=ckpt_path,
            wan_path=wan_path,
            vlm_path=vlm_path,
            device="cuda" if torch.cuda.is_available() else "cpu",
            execute_steps=20,
            stat_path=os.path.join(CURRENT_DIR, "utils", "stat.json")
        )

        B = 2 # Test with batch of 2

        # Test 1: Mock T5 Embedding [B, 512, 4096]
        dummy_t5 = torch.randn((B, 512, 4096), dtype=torch.bfloat16) # dummy umt5-xxl
        dummy_t5_path = "dummy_t5.pt"
        torch.save(dummy_t5, dummy_t5_path)
        policy.set_t5_embedding(dummy_t5_path)

        # Create dummy observation
        dummy_image = np.random.randint(0, 255, (B, 384, 320, 3), dtype=np.uint8)
        dummy_state = np.zeros((B, 9), dtype=np.float32)

        print(f"Requesting first action (should trigger replan) with Batch {B}...")
        action1 = policy.act(dummy_image, dummy_state)
        print(f"Action 1 shape: {action1.shape} (Expected: ({B}, 8))")
        print(f"Queue size after action 1: {len(policy.action_queue)}")

        print("Requesting second action (should pop from queue)...")
        action2 = policy.act(dummy_image, dummy_state)
        print(f"Action 2 shape: {action2.shape} (Expected: ({B}, 8))")
        print(f"Queue size after action 2: {len(policy.action_queue)}")

        print("Verification passed successfully.")
    except Exception as e:
        print(f"Verification could not complete because model files are missing or inaccessible: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    verify()
