common:
  # Robot dimensions
  action_dim: 14
  state_dim: 14

  # Video settings (resize LeRobot dataset video frames to this resolution)
  num_video_frames: 8
  video_height: 384
  video_width: 320

  # Sampling strategy parameters
  global_downsample_rate: 1
  video_action_freq_ratio: 6

# Dataset configuration
dataset:
  type: "lerobot"
  max_episodes: 50
  image_aug: false
  params:
    # Required: LeRobotMotusDataset needs repo_id + root to locate the local dataset
    repo_id: "beat_block_hammer"
    root: "/share/dataset/lerobot/robotwin/beat_block_hammer"

    # When t5_embedding is missing, encode on-the-fly and cache it to {root}/t5_embedding/episode_XXXXXX.pt, also write back to meta/episodes.jsonl
    enable_t5_fallback: true
    t5_wan_path: "/share/home/bhz/pretrained_models"
    t5_folder_name: "t5_embedding"
    t5_text_len: 512

# Model configuration（directly use ac_one default WAN/VLM paths）
model:
  wan:
    config_path: "/share/home/bhz/pretrained_models/Wan2.2-TI2V-5B"
    checkpoint_path: "/share/home/bhz/pretrained_models/Wan2.2-TI2V-5B"
    vae_path: "/share/home/bhz/pretrained_models/Wan2.2-TI2V-5B/Wan2.2_VAE.pth"
    precision: "bfloat16"
  vlm:
    checkpoint_path: "/share/home/bhz/pretrained_models/Qwen3-VL-2B-Instruct"
    precision: "bfloat16"
    frozen: true
  action_expert:
    hidden_size: 1024
    ffn_dim_multiplier: 4
    norm_eps: 1e-5
  und_expert:
    hidden_size: 512
    ffn_dim_multiplier: 4
    norm_eps: 1e-5
    vlm:
      input_dim: 2048
      projector_type: "mlp3x_silu"
  time_distribution:
    timestep_sample_method: "logit_normal"
    sigmoid_scale: 1.0
    min_t: 0.0
    max_t: 1.0
  inference:
    num_inference_timesteps: 10
  loss_weights:
    video_loss_weight: 1.0
    action_loss_weight: 1.0
  ema:
    enabled: false
    update_after_step: 0
    inv_gamma: 1.0
    power: 0.75
    min_value: 0.0
    max_value: 0.9999

# Training configuration
training:
  # Optimization settings
  batch_size: 8
  max_steps: 1000000
  learning_rate: 5.0e-5
  weight_decay: 0.01
  
  # Scheduler settings
  scheduler_type: "linear"
  warmup_steps: 200
  cycle_length: 1000000   # To5al cycle length for scheduler
  f_max: 0.99           # Maximum learning rate multiplier after warmup
  f_min: 0.4            # Minimum learning rate multiplier at end
  
  # Gradient settings
  grad_clip_norm: 0.5
  
  # Mixed precision
  use_amp: true          # Use automatic mixed precision
  
  # Distributed training
  find_unused_parameters: false

# System settings
system:
  # Paths
  checkpoint_dir: "./checkpoints"
  log_level: "INFO"
  
  # Intervals
  log_interval: 1       # Log every N steps
  save_interval: 5000    # Save every N steps  
  val_interval: 500     # Validate every N steps
  
  # Hardware settings
  num_workers: 16         # Number of dataloader workers (0 = main process only, avoids memory issues)
  pin_memory: true       # Pin memory for faster GPU transfer

# Logging settings  
logging:
  # Reporting - support multiple logging backends
  report_to: "wandb"  # Options: "wandb", "tensorboard", "all", "none"
  
  # Weights & Biases settings
  wandb_project: "latent-action-world-model"
  
  # TensorBoard settings
  tensorboard_log_dir: "tensorboard_logs"
  
  # Run naming
  run_name: null  # Optional run name (timestamp will be appended)
  

# Resume training settings
resume:
  checkpoint_path: null  # Path to checkpoint to resume from
  
# Finetune settings 
finetune:
  checkpoint_path: null # (directory containing mp_rank_00_model_states.pt)