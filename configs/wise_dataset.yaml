common:
  # Robot dimensions
  action_dim: 8
  state_dim: 9

  # Video settings (resize LeRobot dataset video frames to this resolution)
  num_video_frames: 8
  video_height: 384
  video_width: 320

  # Sampling strategy parameters
  global_downsample_rate: 1
  video_action_freq_ratio: 6

# Dataset configuration
dataset:
  type: "lerobot"
  max_episodes: null
  image_aug: false
  
  # Task selection parameters
  task_mode: "single"      # "single" or "multi" - single task or multi task training
  # task_name options:
  #   - Single task: task_name: fold_towel
  #   - Multiple specified tasks (multi mode): task_name: [fold_towel, fold_towel_0929]
  #   - All tasks (multi mode): task_name: null
  task_name: null # null means all tasks in the dataset or ["task1", "task2"] for specific tasks

  params:
    # Required: LeRobotMotusDataset needs repo_id + root to locate the local dataset
    repo_id: "wise_dataset_merged"
    root: "/work/vita/lanfeng/vlas/vla/wise_dataset/merged_dataset"

    # Embodiment type for loading normalization statistics (e.g., "aloha_agilex_2", "ac_one")
    embodiment_type: "wise_bot"

    # When t5_embedding is missing, encode on-the-fly and cache it to {root}/t5_embedding/episode_XXXXXX.pt, also write back to meta/episodes.jsonl
    enable_t5_fallback: true
    t5_wan_path: "/work/vita/lanfeng/vlas/Motus/pretrained_models"
    t5_folder_name: "t5_embedding"
    t5_text_len: 512

# Model configuration
model:
  wan:
    config_path: "/work/vita/lanfeng/vlas/Motus/pretrained_models/Wan2.2-TI2V-5B"
    checkpoint_path: "/work/vita/lanfeng/vlas/Motus/pretrained_models/Wan2.2-TI2V-5B"
    vae_path: "/work/vita/lanfeng/vlas/Motus/pretrained_models/Wan2.2-TI2V-5B/Wan2.2_VAE.pth"
    precision: "bfloat16"
  vlm:
    checkpoint_path: "/work/vita/lanfeng/vlas/Motus/pretrained_models/Qwen3-VL-2B-Instruct"
    precision: "bfloat16"
    frozen: true
  action_expert:
    hidden_size: 1024
    ffn_dim_multiplier: 4
    norm_eps: 1e-5
  und_expert:
    hidden_size: 512
    ffn_dim_multiplier: 4
    norm_eps: 1e-5
    vlm:
      input_dim: 2048
      projector_type: "mlp3x_silu"
  time_distribution:
    timestep_sample_method: "logit_normal"
    sigmoid_scale: 1.0
    min_t: 0.0
    max_t: 1.0
  inference:
    num_inference_timesteps: 10
  loss_weights:
    video_loss_weight: 1.0
    action_loss_weight: 1.0
  ema:
    enabled: false
    update_after_step: 0
    inv_gamma: 1.0
    power: 0.75
    min_value: 0.0
    max_value: 0.9999

# Training configuration
training:
  # Optimization settings
  batch_size: 8
  max_steps: 1000000
  learning_rate: 5.0e-5
  weight_decay: 0.01
  
  # Scheduler settings
  scheduler_type: "linear"
  warmup_steps: 200
  cycle_length: 1000000   # To5al cycle length for scheduler
  f_max: 0.99           # Maximum learning rate multiplier after warmup
  f_min: 0.4            # Minimum learning rate multiplier at end
  
  # Gradient settings
  grad_clip_norm: 0.5
  
  # Mixed precision
  use_amp: true          # Use automatic mixed precision
  
  # Distributed training
  find_unused_parameters: false

# System settings
system:
  # Paths
  checkpoint_dir: "./checkpoints"
  log_level: "INFO"
  
  # Intervals
  log_interval: 1       # Log every N steps
  save_interval: 5000    # Save every N steps  
  val_interval: 500     # Validate every N steps
  
  # Hardware settings
  num_workers: 16         # Number of dataloader workers (0 = main process only, 4-8 recommended for speed)
  pin_memory: true       # Pin memory for faster GPU transfer

# Profiler settings (for performance analysis, especially dataset loading)
profiler:
  enable: true          # Enable torch.profiler to track dataset loading and training performance
  interval: 100          # Profiler interval (not used directly, but kept for compatibility)
  warmup_steps: 5        # Number of warmup steps before profiling starts
  active_steps: 5        # Number of active profiling steps per cycle
  repeat_steps: 1000     # Number of steps to wait before repeating profiling cycle

# Logging settings  
logging:
  # Reporting - support multiple logging backends
  report_to: "wandb"  # Options: "wandb", "tensorboard", "all", "none"
  
  # Weights & Biases settings
  wandb_project: "motus"
  
  # TensorBoard settings
  tensorboard_log_dir: "tensorboard_logs"
  
  # Run naming
  run_name: null  # Optional run name (timestamp will be appended)
  

# Resume training settings
resume:
  checkpoint_path: null  # Path to checkpoint to resume from
  
# Finetune settings
finetune:
  checkpoint_path: "/work/vita/lanfeng/vlas/Motus/pretrained_models/Motus"
